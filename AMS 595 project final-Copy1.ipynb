{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a816040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mglearn\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ca0f1",
   "metadata": {},
   "source": [
    "# Train-Test Split Function\n",
    "The `train_test_split` function is responsible for splitting the dataset into training and test sets.\n",
    "\n",
    "- $X$: Input features (matrix).\n",
    "- $y$: Target values (vector).\n",
    "- $test\\_size$: Proportion of the dataset to include in the test split.\n",
    "- $random\\_state$: Seed for the random number generator, ensuring reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd9e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    \n",
    "    # Set random seed for reproducibility if random_state is provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        \n",
    "    # Get the total number of samples\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    \n",
    "    # Create an array of indices and shuffle them\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Determine the number of samples for the test set\n",
    "    if type(test_size) == float:\n",
    "        test_size = int(test_size * n_samples)\n",
    "        \n",
    "        \n",
    "    # Extract indices for the test and training sets\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    \n",
    "    \n",
    "    # Use indices to split the data into training and testing sets\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d631d7",
   "metadata": {},
   "source": [
    "# Linear Regression Functions\n",
    "\n",
    "\n",
    "## Coefficient calculation formula\n",
    "The coefficients for linear regression are obtained using the normal equation:\n",
    "$$\n",
    "\\text{coefficients} = (X_{\\text{ext}}^T X_{\\text{ext}})^{-1} X_{\\text{ext}}^T y\n",
    "$$\n",
    "\n",
    "## Linear Regression Prediction formula\n",
    "The prediction for linear regression is given by:\n",
    "$$\n",
    "y_{\\text{pred}} = X_{\\text{ext}} \\cdot \\text{coefficients}\n",
    "$$\n",
    "\n",
    "\n",
    "## Fitting Linear Regression\n",
    "The `fit_linear_regression` function calculates the coefficients for linear regression using the normal equation.\n",
    "\n",
    "- $X$: Input features.\n",
    "- $y$: Target values.\n",
    "- $X_{\\text{ext}}$: Extended feature matrix with an added column of ones for the intercept term.\n",
    "- $\\text{coefficients}$: Coefficients for the intercept and features.\n",
    "\n",
    "## Prediction\n",
    "The `predict` function predicts target values based on input features and coefficients.\n",
    "\n",
    "- $X$: Input features.\n",
    "- $X_{\\text{ext}}$: Extended feature matrix.\n",
    "- $\\text{coefficients}$: Coefficients obtained from linear regression.\n",
    "- $y_{\\text{pred}}$: Predicted target values.\n",
    "\n",
    "## Mean Squared Error (MSE) formula\n",
    "The Mean Squared Error (MSE) is calculated as follows:\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true},i} - y_{\\text{pred},i})^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Mean Squared Error Calculation\n",
    "The `mean_squared_error` function calculates the mean squared error between true and predicted values.\n",
    "\n",
    "- $y_{\\text{true}}$: True target values.\n",
    "- $y_{\\text{pred}}$: Predicted target values.\n",
    "- $\\text{mse}$: Mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca22123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_ext = np.column_stack((np.ones(len(X)), X))\n",
    "    \n",
    "    # Calculate coefficients using the normal equation\n",
    "    coefficients = np.dot(np.dot(np.linalg.pinv(np.dot(X_ext.T, X_ext)), X_ext.T), y)\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "def predict(X, coefficients):\n",
    "    \n",
    "    \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_ext = np.column_stack((np.ones(len(X)), X))\n",
    "    \n",
    "    # Calculate predicted target values using dot product\n",
    "    y_pred = np.dot(X_ext, coefficients)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f4340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \n",
    "    \n",
    "    # Ensure that the input arrays have the same length\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "\n",
    "    # Calculate squared differences\n",
    "    squared_diff = []\n",
    "    for i in range(len(y_pred)):\n",
    "        squared_diff.append((y_true[i] - y_pred[i]) ** 2)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = sum(squared_diff) / len(y_true)\n",
    "\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9353fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training Set: 5.119969179921958\n",
      "Mean Squared Error on Test Set: 14.329434192452062\n"
     ]
    }
   ],
   "source": [
    "# Load the extended Boston Housing dataset\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "coefficients = fit_linear_regression(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = predict(X_train, coefficients)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(X_test, coefficients)\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Mean Squared Error on Training Set:\", mse_train)\n",
    "print(\"Mean Squared Error on Test Set:\", mse_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57df55e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and test sets\n",
    "mse_values = [mse_train, mse_test]\n",
    "set_labels = ['Training Set', 'Test Set']\n",
    "\n",
    "# Create a bar plot to compare MSE values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(set_labels, mse_values, color=['blue', 'green'])\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE Comparison between Training and Test Sets')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a6381",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge regression, also known as Tikhonov regularization or L2 regularization, takes the least squares function and adds a regularization term to it. This is typically used to prevent overfitting.\n",
    "\n",
    "The ridge regression function is given by:\n",
    "\n",
    "$$\n",
    "\\text{J(θ)} = MSE(θ) + α\\sum_{i=1}^{n} (θ_i)^2 \n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{J(θ)}$ is the cost function to be minimized.\n",
    "- $MSE(θ)$ is the Mean Squared Error term.\n",
    "- $α$ is the regularization parameter.\n",
    "- $θ_i$ are the regression coefficients.\n",
    "\n",
    "The ridge regression coefficients are obtained by minimizing this cost function:\n",
    "\n",
    "$$\n",
    "\\hat{θ} = \\text{argmin}_θ J(θ)\n",
    "$$\n",
    "\n",
    "The closed-form solution for ridge regression is given by:\n",
    "\n",
    "$$\n",
    "\\hat{θ} = (X^T X + αI)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\hat{θ}$ is the vector of ridge regression coefficients.\n",
    "- $X$ is the matrix of input features.\n",
    "- $y$ is the vector of target values.\n",
    "- $I$ is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50b5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# closed-form approach\n",
    "def RidgeRegression(X, Y, alpha):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    I = np.identity(X_b.shape[1])\n",
    "    I[0, 0] = 0\n",
    "    \n",
    "    theta = np.linalg.inv(X_b.T.dot(X_b) + alpha * I).dot(X_b.T).dot(Y)\n",
    "    \n",
    "    # Separate bias and weights\n",
    "    w = theta[1:]\n",
    "    b = theta[0]\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a7a00",
   "metadata": {},
   "source": [
    "Iterative methods like gradient descent are also used for ridge regression.\n",
    "\n",
    "The cost function can also be written as:\n",
    "\n",
    "$$\n",
    "\\text{J(θ)} = \\frac{1}{2m}(\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})^2 + α\\sum_{j=1}^{n}(θ_j)^2)\n",
    "$$\n",
    "\n",
    "with the derivative with respect to $w_j$ (weights) when $θ_j$, $j \\ge 1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ)}{\\partial w_j} = \\frac{1}{m}(\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} + α\\cdot w_j)\n",
    "$$\n",
    "\n",
    "with the derivative with respect to $b$ (bias) when $θ_0$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ)}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples.\n",
    "- $h_θ(x^{(i)})$ is the predicted value of the $i$-th example.\n",
    "- $y^{(i)}$ is the actual value of the $i$-th example.\n",
    "- $θ_j$ are the regression coefficients.\n",
    "- $n$ is the number of input features.\n",
    "- $α$ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d10894b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent approach\n",
    "def gdRR(X, y, alpha, lr, n_iterations):\n",
    "    m, n = X.shape\n",
    "    W = np.zeros(n)\n",
    "    b = 0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        pred = np.dot(X, W) + b\n",
    "        diff = pred - y\n",
    "        dJdW = (1/m) * np.dot(X.T, diff) + alpha * W\n",
    "        dJdb = (1/m) * np.sum(diff)\n",
    "        \n",
    "        W -= lr * dJdW\n",
    "        b -= lr * dJdb\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a922a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.60144060315256, 60.67839785420658, 61.50384459946837, 72.37612588307135, 116.77088424301623, 124.3714659334745, 1.2713103417502642e+190]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>w</th>\n",
       "      <th>b</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5.288569</td>\n",
       "      <td>-25.050419</td>\n",
       "      <td>13.519711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.665918</td>\n",
       "      <td>-14.889592</td>\n",
       "      <td>12.257621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>-3.472767</td>\n",
       "      <td>6.438892</td>\n",
       "      <td>11.013404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.493739</td>\n",
       "      <td>19.766685</td>\n",
       "      <td>12.710819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-0.856798</td>\n",
       "      <td>23.426068</td>\n",
       "      <td>20.179910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.000</td>\n",
       "      <td>-0.293358</td>\n",
       "      <td>25.241065</td>\n",
       "      <td>33.025438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>-0.086045</td>\n",
       "      <td>24.776840</td>\n",
       "      <td>53.970180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Alpha         w          b        MSE\n",
       "0     0.001  5.288569 -25.050419  13.519711\n",
       "1     0.010 -5.665918 -14.889592  12.257621\n",
       "2     0.100 -3.472767   6.438892  11.013404\n",
       "3     1.000 -1.493739  19.766685  12.710819\n",
       "4    10.000 -0.856798  23.426068  20.179910\n",
       "5   100.000 -0.293358  25.241065  33.025438\n",
       "6  1000.000 -0.086045  24.776840  53.970180"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "results = []\n",
    "w_set = []\n",
    "b_set = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "  w, b = RidgeRegression(X_train, y_train, alpha)\n",
    "  Y_pred = np.dot(X_test, w) + b\n",
    "\n",
    "  results.append(mean_squared_error(y_test, Y_pred))\n",
    "  w_set.append(w)\n",
    "  b_set.append(b)\n",
    "\n",
    "w_set = [w[0] for w in w_set]\n",
    "\n",
    "results_gd = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    w1, b1 = gdRR(X_train, y_train, alpha, lr = 0.01, n_iterations = 100)\n",
    "    Y_pred = np.dot(X_test, w1) + b1\n",
    "    results_gd.append(mean_squared_error(y_test, Y_pred))\n",
    "\n",
    "print(results_gd)\n",
    "\n",
    "data = {\"Alpha\": test_alphas, \"w\": w_set, \"b\": b_set, \"MSE\": results}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be0dfc",
   "metadata": {},
   "source": [
    "From the above table, it can be seen that the lowest MSE for this dataset is when α = 0.1, which is our optimal regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c68e7",
   "metadata": {},
   "source": [
    "The ridge regression function can be found from sklearn.linear_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6af6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 13.519710896804316\n",
      "Mean Squared Error: 12.25762120221095\n",
      "Mean Squared Error: 11.01340443725933\n",
      "Mean Squared Error: 12.710819494074418\n",
      "Mean Squared Error: 20.17990981392929\n",
      "Mean Squared Error: 33.02543777154453\n",
      "Mean Squared Error: 53.970179940917824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "\n",
    "    # Train the model\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee5215",
   "metadata": {},
   "source": [
    "As you can see, the MSE of the different alpha levels match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fecc7a",
   "metadata": {},
   "source": [
    "## LASSO Regression\n",
    "\n",
    "LASSO stands for Least Absolute Shrinkage and Selection Operator, is a linear regression technique with added regularization. It combines the simplicity of linear regression with the regularization power of the L1 norm to prevent overfitting and perform feature selection.\n",
    "\n",
    "The cost function of LASSO regression is given by:\n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + α\\sum_{i=1}^{n} |\\theta_i|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{J(θ)}$ is the cost function to be minimized.\n",
    "- $MSE(θ)$ is the Mean Squared Error term.\n",
    "- $α$ is the regularization parameter.\n",
    "- $θ_i$ are the regression coefficients.\n",
    "\n",
    "The LASSO regression coefficients are obtained by minimizing this cost function:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\text{argmin}_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "There is no general closed-form solution for LASSO regression but is commonly minimized through gradient descent.\n",
    "\n",
    "Our cost function can also be written as:\n",
    "\n",
    "$$\n",
    "J(\\hat{\\theta})=\\frac{1}{2m} \\big|\\big| Y-\\hat{\\theta}X \\big|\\big| + \\alpha∑_{i=1}^{n} \\hat{|\\theta_i|}\n",
    "$$\n",
    "\n",
    "We are looking to minimize the cost fuction with respect to W and b\n",
    "Where $\\hat{\\theta} = \\begin{bmatrix}  b & {\\hat{\\theta_1}} &.&.&.& {\\hat{\\theta_n}} \\end{bmatrix}$\n",
    "and $W = \\begin{bmatrix} \\hat{\\theta}_1 &.&.&.& \\hat{\\theta_n} \\end{bmatrix}$\n",
    "\n",
    "so $\\frac{d}{dW}J(\\hat{\\theta}) = \\frac{1}{m}X^T(Y-\\hat{\\theta}X) + \\alpha sgn(W)$\n",
    "\n",
    "and $\\frac{d}{db}J(\\hat{\\theta}) = \\frac{1}{m}(Y-\\hat{\\theta}X)$\n",
    "\n",
    "Then using the gradient descent:\n",
    "$$\n",
    "\\theta_{k+1}=\\theta_{k}-h \\nabla J\\left(\\theta_{k}\\right)\n",
    "$$\n",
    "\n",
    "Where $\\theta_{k}$ is our initial guess and $\\theta_{k+1}$ is our new guess and $h$ is our step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f9305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(X, y, alpha, num_iterations, learning_rate):\n",
    "\n",
    "    # Get m the number of observations and n the number of independent variables\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    W = np.zeros(n)\n",
    "\n",
    "    # Initialize intercept term\n",
    "    b = 0\n",
    "\n",
    "    # Performing gradient descent to find the minimum of out cost function\n",
    "    for iteration in range(num_iterations):\n",
    "        predictions = np.dot(X, W) + b\n",
    "        residuals = predictions - y\n",
    "        dJdW = (1/m)*np.dot(X.T, residuals) + alpha * np.sign(W)\n",
    "        dJdb = (1/m)*np.sum(residuals)\n",
    "\n",
    "        W -= learning_rate * dJdW\n",
    "        b -= learning_rate * dJdb\n",
    "\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c7b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.357483029837827, 12.370242247366956, 22.463406393328086, 54.92725786833492, 67.1032788247271, 135.22114084965105, 518498.2318359897]\n"
     ]
    }
   ],
   "source": [
    "las_results = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "  w, b = lasso_regression(X_train, y_train, alpha,num_iterations = 100000, learning_rate = 0.05)\n",
    "  Y_pred = np.dot(X_test, w) + b\n",
    "\n",
    "  las_results.append(mean_squared_error(y_test, Y_pred))\n",
    "\n",
    "print(las_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47094ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11.413035283804978\n",
      "Mean Squared Error: 12.901685764529898\n",
      "Mean Squared Error: 22.456548850036334\n",
      "Mean Squared Error: 55.313519821949356\n",
      "Mean Squared Error: 75.76013971789499\n",
      "Mean Squared Error: 75.76013971789499\n",
      "Mean Squared Error: 75.76013971789499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    lasso_model = Lasso(alpha=alpha,max_iter = 100000)\n",
    "\n",
    "    # Train the model\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ceb8dc",
   "metadata": {},
   "source": [
    "We can see that up to alpha = 10 the MSE of our gradient descent is approximately equal to the sklearn LASSO MSE. This will differ by changing up the number of iterations and learning rate.\n",
    "\n",
    "Also alpha = 0.001 or 0.01 seems to minimize the MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e207713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.05065185e-03 -1.30786474e-04  1.33800659e-03  2.89429150e-03\n",
      "  2.73764867e-03  8.84182765e-03 -1.54815999e-03 -8.63094201e+00\n",
      "  1.32791234e+01  9.55667847e-03  2.14979186e-03  4.19764849e+00\n",
      " -6.85762349e+00 -4.63692482e-04 -1.86216142e-04 -9.35529136e-04\n",
      " -6.00485784e-04 -1.05654121e-03  1.46659907e-03 -2.51719760e-03\n",
      " -1.81941515e-03 -8.09130848e+00 -3.15661566e-03 -8.94036386e-04\n",
      " -3.45122570e-03 -2.22740287e-04  1.80581780e+00 -1.62805211e-03\n",
      "  1.03268103e-03 -1.80140876e-03  3.51369019e-03  3.16503774e-04\n",
      "  1.91503091e-04 -1.07491103e-03  7.81459824e-04  1.44159791e-03\n",
      "  1.16219890e-03 -1.56990382e-03  2.18878756e-02  1.67475186e-03\n",
      "  2.21389775e-03  1.32967725e-03  7.09162498e-03 -7.95280675e-01\n",
      "  6.03134171e-03  7.83502016e+00 -3.75918116e-03  6.89980854e-03\n",
      " -3.76016918e-05  2.89429150e-03 -6.48213890e+00 -2.22383922e+00\n",
      "  1.43351372e-03  1.12349763e-03  6.97753137e-04  9.61843749e+00\n",
      "  1.05470690e-04  2.20876328e+00  2.29266121e-04 -2.48713144e+00\n",
      " -3.09323558e+00 -1.59183829e-03 -1.25676189e-03 -3.59282200e-05\n",
      " -1.08211790e-03 -4.73416243e+00  3.67942588e-03 -6.94777223e-04\n",
      "  3.70381016e+01 -6.34780919e-01  3.80406616e-03 -2.10880669e+01\n",
      " -9.96762804e+00 -7.05916529e+00  1.51027443e+00  4.45601468e-04\n",
      "  1.02327299e-02 -1.82181418e+00  7.23671941e+00  6.00478842e-03\n",
      " -1.45926234e+00 -6.43736544e-04 -8.84050144e+00  9.83366693e-04\n",
      " -1.85603329e-05  2.73256214e-04  1.39739994e-03  2.96523991e-04\n",
      " -1.07653061e-04  7.81499137e-03  8.31458907e-03  4.86580644e-03\n",
      "  1.43432176e+00 -1.51581589e+01  6.85886437e-03  4.92662955e-03\n",
      "  5.44466284e-03 -1.33568534e+01  8.22065898e-03  9.15026123e-03\n",
      "  2.11520011e-03  1.15686572e-02 -7.86835969e+00  2.12180860e+01]\n",
      "19.302661954296124\n"
     ]
    }
   ],
   "source": [
    "w, b = lasso_regression(X_train, y_train, alpha=0.01,num_iterations = 200000, learning_rate = 0.2)\n",
    "\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bafa3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -8.63094201e+00\n",
      "  1.32791234e+01  0.00000000e+00  0.00000000e+00  4.19764849e+00\n",
      " -6.85762349e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -8.09130848e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.80581780e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  2.18878756e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.95280675e-01\n",
      "  0.00000000e+00  7.83502016e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.48213890e+00 -2.22383922e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  9.61843749e+00\n",
      "  0.00000000e+00  2.20876328e+00  0.00000000e+00 -2.48713144e+00\n",
      " -3.09323558e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -4.73416243e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.70381016e+01 -6.34780919e-01  0.00000000e+00 -2.10880669e+01\n",
      " -9.96762804e+00 -7.05916529e+00  1.51027443e+00  0.00000000e+00\n",
      "  1.02327299e-02 -1.82181418e+00  7.23671941e+00  0.00000000e+00\n",
      " -1.45926234e+00  0.00000000e+00 -8.84050144e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.43432176e+00 -1.51581589e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.33568534e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.15686572e-02 -7.86835969e+00  2.12180860e+01]\n"
     ]
    }
   ],
   "source": [
    "# Get ride of small values close to 0\n",
    "for i in range(len(w)):\n",
    "    if(abs(w[i]) < 0.01):\n",
    "        w[i] = 0\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "571817fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -8.61448752e+00\n",
      "  1.32747817e+01  0.00000000e+00 -0.00000000e+00  4.22948747e+00\n",
      " -6.79244783e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -8.10113946e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  1.80657247e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  7.84771010e-03  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -8.13870499e-01\n",
      "  0.00000000e+00  7.87205046e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -6.47720077e+00 -2.21214790e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  9.61987786e+00\n",
      "  0.00000000e+00  2.20457393e+00 -0.00000000e+00 -2.49419166e+00\n",
      " -3.13735059e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -4.71374387e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  3.70600056e+01 -6.22026316e-01  0.00000000e+00 -2.11053722e+01\n",
      " -9.95546028e+00 -7.05137004e+00  1.47924858e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -1.85318578e+00  7.23916984e+00  0.00000000e+00\n",
      " -1.47972146e+00 -0.00000000e+00 -8.82302770e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.43952155e+00 -1.51421719e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.34045830e+01  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -7.91571342e+00  2.11787418e+01]\n",
      "19.26548527425806\n"
     ]
    }
   ],
   "source": [
    "# Create a Lasso regression model\n",
    "lasso_model = Lasso(alpha=0.01,max_iter = 1000000)\n",
    "# Fit the model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "print(lasso_model.coef_)\n",
    "print(lasso_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c416877",
   "metadata": {},
   "source": [
    "If we compare the coefficients of sklearn LASSO to the gradient descent, the gradient descent has almost the same values as the sklearn one\n",
    "\n",
    "But notice that we had to change the number of iterations and learning rate to find a close approximate to the sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10092e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.890890065621827\n"
     ]
    }
   ],
   "source": [
    "Y_pred_test = np.dot(X_test, w) + b\n",
    "print(mean_squared_error(y_test, Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f5d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5147286211851\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = np.dot(X_train, w) + b\n",
    "print(mean_squared_error(y_train, Y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c71b7",
   "metadata": {},
   "source": [
    "The MSE now for our LASSO function is 12.295366661959434 now further from the SKlearn MSE of 12.901685764529885 but the coeffcients are now closer.\n",
    "\n",
    "Our training data MSE and test MSE now seem to be much closer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
