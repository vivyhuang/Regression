{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6bd9e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training Set: 5.1199691799219735\n",
      "Mean Squared Error on Test Set: 14.32943419266377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/v/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import mglearn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    \n",
    "    # Set random seed for reproducibility if random_state is provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        \n",
    "    # Get the total number of samples\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    \n",
    "    # Create an array of indices and shuffle them\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Determine the number of samples for the test set\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(test_size * n_samples)\n",
    "        \n",
    "        \n",
    "    # Extract indices for the test and training sets\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    \n",
    "    \n",
    "    # Use indices to split the data into training and testing sets\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def fit_linear_regression(X, y):\n",
    "    \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_ext = np.column_stack((np.ones(len(X)), X))\n",
    "    \n",
    "    # Calculate coefficients using the normal equation\n",
    "    coefficients = np.dot(np.dot(np.linalg.pinv(np.dot(X_ext.T, X_ext)), X_ext.T), y)\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "def predict(X, coefficients):\n",
    "    \n",
    "    \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_ext = np.column_stack((np.ones(len(X)), X))\n",
    "    \n",
    "    # Calculate predicted target values using dot product\n",
    "    y_pred = np.dot(X_ext, coefficients)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \n",
    "    \n",
    "    # Ensure that the input arrays have the same length\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "\n",
    "    # Calculate squared differences\n",
    "    squared_diff = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = sum(squared_diff) / len(y_true)\n",
    "\n",
    "    return mse\n",
    "# Load the extended Boston Housing dataset\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "coefficients = fit_linear_regression(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = predict(X_train, coefficients)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(X_test, coefficients)\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Mean Squared Error on Training Set:\", mse_train)\n",
    "print(\"Mean Squared Error on Test Set:\", mse_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a6381",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge regression, also known as Tikhonov regularization or L2 regularization, takes the least squares function and adds a regularization term to it. This is typically used to prevent overfitting.\n",
    "\n",
    "The ridge regression function is given by:\n",
    "\n",
    "$$\n",
    "\\text{J(θ)} = MSE(θ) + α\\sum_{i=1}^{n} (θ_i)^2 \n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{J(θ)}$ is the cost function to be minimized.\n",
    "- $MSE(θ)$ is the Mean Squared Error term.\n",
    "- $α$ is the regularization parameter.\n",
    "- $θ_i$ are the regression coefficients.\n",
    "\n",
    "The ridge regression coefficients are obtained by minimizing this cost function:\n",
    "\n",
    "$$\n",
    "\\hat{θ} = \\text{argmin}_θ J(θ)\n",
    "$$\n",
    "\n",
    "The closed-form solution for ridge regression is given by:\n",
    "\n",
    "$$\n",
    "\\hat{θ} = (X^T X + αI)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\hat{θ}$ is the vector of ridge regression coefficients.\n",
    "- $X$ is the matrix of input features.\n",
    "- $y$ is the vector of target values.\n",
    "- $I$ is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b50b5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# closed-form approach\n",
    "def RidgeRegression(X, Y, alpha):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    I = np.identity(X_b.shape[1])\n",
    "    I[0, 0] = 0\n",
    "    \n",
    "    theta = np.linalg.inv(X_b.T.dot(X_b) + alpha * I).dot(X_b.T).dot(Y)\n",
    "    \n",
    "    # Separate bias and weights\n",
    "    w = theta[1:]\n",
    "    b = theta[0]\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a7a00",
   "metadata": {},
   "source": [
    "Iterative methods like gradient descent are also used for ridge regression.\n",
    "\n",
    "The cost function can also be written as:\n",
    "\n",
    "$$\n",
    "\\text{J(θ)} = \\frac{1}{2m}(\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})^2 + α\\sum_{j=1}^{n}(θ_j)^2)\n",
    "$$\n",
    "\n",
    "with the derivative with respect to $w_j$ (weights) when $θ_j$, $j \\ge 1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ)}{\\partial w_j} = \\frac{1}{m}(\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} + α\\cdot w_j)\n",
    "$$\n",
    "\n",
    "with the derivate with respect to $b$ (bias) when $θ_0$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ)}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples.\n",
    "- $h_θ(x^{(i)})$ is the predicted value of the $i$-th example.\n",
    "- $y^{(i)}$ is the actual value of the $i$-th example.\n",
    "- $θ_j$ are the regression coefficients.\n",
    "- $n$ is the number of input features.\n",
    "- $α$ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d10894b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent approach\n",
    "def gdRR(X, y, alpha, lr, n_iterations):\n",
    "    m, n = X.shape\n",
    "    W = np.zeros(n)\n",
    "    b = 0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        pred = np.dot(X, W) + b\n",
    "        diff = pred - y\n",
    "        dJdW = (1/m) * np.dot(X.T, diff) + alpha * W\n",
    "        dJdb = (1/m) * np.sum(diff)\n",
    "        \n",
    "        W -= lr * dJdW\n",
    "        b -= lr * dJdb\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9a922a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.601440603152554, 60.67839785420658, 61.50384459946839, 72.37612588307135, 116.77088424301624, 124.37146593347452, 1.2713103417502652e+190]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>w</th>\n",
       "      <th>b</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5.288569</td>\n",
       "      <td>-25.050419</td>\n",
       "      <td>13.519711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.665918</td>\n",
       "      <td>-14.889592</td>\n",
       "      <td>12.257621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>-3.472767</td>\n",
       "      <td>6.438892</td>\n",
       "      <td>11.013404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.493739</td>\n",
       "      <td>19.766685</td>\n",
       "      <td>12.710819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-0.856798</td>\n",
       "      <td>23.426068</td>\n",
       "      <td>20.179910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.000</td>\n",
       "      <td>-0.293358</td>\n",
       "      <td>25.241065</td>\n",
       "      <td>33.025438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>-0.086045</td>\n",
       "      <td>24.776840</td>\n",
       "      <td>53.970180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Alpha         w          b        MSE\n",
       "0     0.001  5.288569 -25.050419  13.519711\n",
       "1     0.010 -5.665918 -14.889592  12.257621\n",
       "2     0.100 -3.472767   6.438892  11.013404\n",
       "3     1.000 -1.493739  19.766685  12.710819\n",
       "4    10.000 -0.856798  23.426068  20.179910\n",
       "5   100.000 -0.293358  25.241065  33.025438\n",
       "6  1000.000 -0.086045  24.776840  53.970180"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "results = []\n",
    "w_set = []\n",
    "b_set = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "  w, b = RidgeRegression(X_train, y_train, alpha)\n",
    "  Y_pred = np.dot(X_test, w) + b\n",
    "\n",
    "  results.append(mean_squared_error(y_test, Y_pred))\n",
    "  w_set.append(w)\n",
    "  b_set.append(b)\n",
    "\n",
    "w_set = [w[0] for w in w_set]\n",
    "\n",
    "results_gd = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    w1, b1 = gdRR(X_train, y_train, alpha, lr = 0.01, n_iterations = 100)\n",
    "    Y_pred = np.dot(X_test, w1) + b1\n",
    "    results_gd.append(mean_squared_error(y_test, Y_pred))\n",
    "\n",
    "print(results_gd)\n",
    "\n",
    "data = {\"Alpha\": test_alphas, \"w\": w_set, \"b\": b_set, \"MSE\": results}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be0dfc",
   "metadata": {},
   "source": [
    "From the above table, it can be seen that the lowest MSE for this dataset is when α = 0.1, which is our optimal regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c68e7",
   "metadata": {},
   "source": [
    "The ridge regression function can be found from sklearn.linear_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3a6af6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 13.519710896809798\n",
      "Mean Squared Error: 12.257621202213597\n",
      "Mean Squared Error: 11.013404437259275\n",
      "Mean Squared Error: 12.710819494074418\n",
      "Mean Squared Error: 20.179909813929285\n",
      "Mean Squared Error: 33.025437771544546\n",
      "Mean Squared Error: 53.970179940917824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "\n",
    "    # Train the model\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee5215",
   "metadata": {},
   "source": [
    "As you can see, the MSE of the different alpha levels match."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
