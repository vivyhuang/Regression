{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bd9e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training Set: 5.119969179921979\n",
      "Mean Squared Error on Test Set: 14.329434191662344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import mglearn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    \n",
    "    # Set random seed for reproducibility if random_state is provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        \n",
    "    # Get the total number of samples\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    \n",
    "    # Create an array of indices and shuffle them\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Determine the number of samples for the test set\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(test_size * n_samples)\n",
    "        \n",
    "        \n",
    "    # Extract indices for the test and training sets\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    \n",
    "    \n",
    "    # Use indices to split the data into training and testing sets\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def fit_linear_regression(X, y):\n",
    "    \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_ext = np.column_stack((np.ones(len(X)), X))\n",
    "    \n",
    "    # Calculate coefficients using the normal equation\n",
    "    coefficients = np.dot(np.dot(np.linalg.pinv(np.dot(X_ext.T, X_ext)), X_ext.T), y)\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "def predict(X, coefficients):\n",
    "    \n",
    "    \n",
    "    # Add a column of ones to X for the intercept term\n",
    "    X_ext = np.column_stack((np.ones(len(X)), X))\n",
    "    \n",
    "    # Calculate predicted target values using dot product\n",
    "    y_pred = np.dot(X_ext, coefficients)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \n",
    "    \n",
    "    # Ensure that the input arrays have the same length\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "\n",
    "    # Calculate squared differences\n",
    "    squared_diff = [(true - pred) ** 2 for true, pred in zip(y_true, y_pred)]\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = sum(squared_diff) / len(y_true)\n",
    "\n",
    "    return mse\n",
    "# Load the extended Boston Housing dataset\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "coefficients = fit_linear_regression(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = predict(X_train, coefficients)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(X_test, coefficients)\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Mean Squared Error on Training Set:\", mse_train)\n",
    "print(\"Mean Squared Error on Test Set:\", mse_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a6381",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge regression, also known as Tikhonov regularization or L2 regularization, takes the least squares function and adds a regularization term to it. This is typically used to prevent overfitting.\n",
    "\n",
    "The ridge regression function is given by:\n",
    "\n",
    "$$\n",
    "\\text{J(θ)} = MSE(θ) + α\\sum_{i=1}^{n} (θ_i)^2 \n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{J(θ)}$ is the cost function to be minimized.\n",
    "- $MSE(θ)$ is the Mean Squared Error term.\n",
    "- $α$ is the regularization parameter.\n",
    "- $θ_i$ are the regression coefficients.\n",
    "\n",
    "The ridge regression coefficients are obtained by minimizing this cost function:\n",
    "\n",
    "$$\n",
    "\\hat{θ} = \\text{argmin}_θ J(θ)\n",
    "$$\n",
    "\n",
    "The closed-form solution for ridge regression is given by:\n",
    "\n",
    "$$\n",
    "\\hat{θ} = (X^T X + αI)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\hat{θ}$ is the vector of ridge regression coefficients.\n",
    "- $X$ is the matrix of input features.\n",
    "- $y$ is the vector of target values.\n",
    "- $I$ is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b50b5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# closed-form approach\n",
    "def RidgeRegression(X, Y, alpha):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    I = np.identity(X_b.shape[1])\n",
    "    I[0, 0] = 0\n",
    "    \n",
    "    theta = np.linalg.inv(X_b.T.dot(X_b) + alpha * I).dot(X_b.T).dot(Y)\n",
    "    \n",
    "    # Separate bias and weights\n",
    "    w = theta[1:]\n",
    "    b = theta[0]\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a7a00",
   "metadata": {},
   "source": [
    "Iterative methods like gradient descent are also used for ridge regression.\n",
    "\n",
    "The cost function can also be written as:\n",
    "\n",
    "$$\n",
    "\\text{J(θ)} = \\frac{1}{2m}(\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})^2 + α\\sum_{j=1}^{n}(θ_j)^2)\n",
    "$$\n",
    "\n",
    "with the derivative with respect to $w_j$ (weights) when $θ_j$, $j \\ge 1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ)}{\\partial w_j} = \\frac{1}{m}(\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} + α\\cdot w_j)\n",
    "$$\n",
    "\n",
    "with the derivative with respect to $b$ (bias) when $θ_0$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ)}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m} (h_θ(x^{(i)})-y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples.\n",
    "- $h_θ(x^{(i)})$ is the predicted value of the $i$-th example.\n",
    "- $y^{(i)}$ is the actual value of the $i$-th example.\n",
    "- $θ_j$ are the regression coefficients.\n",
    "- $n$ is the number of input features.\n",
    "- $α$ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d10894b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent approach\n",
    "def gdRR(X, y, alpha, lr, n_iterations):\n",
    "    m, n = X.shape\n",
    "    W = np.zeros(n)\n",
    "    b = 0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        pred = np.dot(X, W) + b\n",
    "        diff = pred - y\n",
    "        dJdW = (1/m) * np.dot(X.T, diff) + alpha * W\n",
    "        dJdb = (1/m) * np.sum(diff)\n",
    "        \n",
    "        W -= lr * dJdW\n",
    "        b -= lr * dJdb\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a922a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.60144060315257, 60.67839785420657, 61.50384459946835, 72.37612588307135, 116.77088424301623, 124.3714659334745, 1.2713103417502642e+190]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>w</th>\n",
       "      <th>b</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>5.288569</td>\n",
       "      <td>-25.050419</td>\n",
       "      <td>13.519711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.665918</td>\n",
       "      <td>-14.889592</td>\n",
       "      <td>12.257621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>-3.472767</td>\n",
       "      <td>6.438892</td>\n",
       "      <td>11.013404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.493739</td>\n",
       "      <td>19.766685</td>\n",
       "      <td>12.710819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-0.856798</td>\n",
       "      <td>23.426068</td>\n",
       "      <td>20.179910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.000</td>\n",
       "      <td>-0.293358</td>\n",
       "      <td>25.241065</td>\n",
       "      <td>33.025438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>-0.086045</td>\n",
       "      <td>24.776840</td>\n",
       "      <td>53.970180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Alpha         w          b        MSE\n",
       "0     0.001  5.288569 -25.050419  13.519711\n",
       "1     0.010 -5.665918 -14.889592  12.257621\n",
       "2     0.100 -3.472767   6.438892  11.013404\n",
       "3     1.000 -1.493739  19.766685  12.710819\n",
       "4    10.000 -0.856798  23.426068  20.179910\n",
       "5   100.000 -0.293358  25.241065  33.025438\n",
       "6  1000.000 -0.086045  24.776840  53.970180"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "results = []\n",
    "w_set = []\n",
    "b_set = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "  w, b = RidgeRegression(X_train, y_train, alpha)\n",
    "  Y_pred = np.dot(X_test, w) + b\n",
    "\n",
    "  results.append(mean_squared_error(y_test, Y_pred))\n",
    "  w_set.append(w)\n",
    "  b_set.append(b)\n",
    "\n",
    "w_set = [w[0] for w in w_set]\n",
    "\n",
    "results_gd = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    w1, b1 = gdRR(X_train, y_train, alpha, lr = 0.01, n_iterations = 100)\n",
    "    Y_pred = np.dot(X_test, w1) + b1\n",
    "    results_gd.append(mean_squared_error(y_test, Y_pred))\n",
    "\n",
    "print(results_gd)\n",
    "\n",
    "data = {\"Alpha\": test_alphas, \"w\": w_set, \"b\": b_set, \"MSE\": results}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be0dfc",
   "metadata": {},
   "source": [
    "From the above table, it can be seen that the lowest MSE for this dataset is when α = 0.1, which is our optimal regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c68e7",
   "metadata": {},
   "source": [
    "The ridge regression function can be found from sklearn.linear_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a6af6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 13.519710896811109\n",
      "Mean Squared Error: 12.257621202214008\n",
      "Mean Squared Error: 11.013404437259284\n",
      "Mean Squared Error: 12.710819494074434\n",
      "Mean Squared Error: 20.17990981392928\n",
      "Mean Squared Error: 33.02543777154453\n",
      "Mean Squared Error: 53.970179940917824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "\n",
    "    # Train the model\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee5215",
   "metadata": {},
   "source": [
    "As you can see, the MSE of the different alpha levels match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fecc7a",
   "metadata": {},
   "source": [
    "## LASSO Regression\n",
    "\n",
    "LASSO stands for Least Absolute Shrinkage and Selection Operator, is a linear regression technique with added regularization. It combines the simplicity of linear regression with the regularization power of the L1 norm to prevent overfitting and perform feature selection.\n",
    "\n",
    "The cost function of LASSO regression is given by:\n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + α\\sum_{i=1}^{n} |\\theta_i|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{J(θ)}$ is the cost function to be minimized.\n",
    "- $MSE(θ)$ is the Mean Squared Error term.\n",
    "- $α$ is the regularization parameter.\n",
    "- $θ_i$ are the regression coefficients.\n",
    "\n",
    "The LASSO regression coefficients are obtained by minimizing this cost function:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\text{argmin}_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "There is no general closed-form solution for LASSO regression but is commonly minimized through gradient descent.\n",
    "\n",
    "Our cost function can also be written as:\n",
    "\n",
    "$$\n",
    "J(\\hat{\\theta})=\\frac{1}{2m} \\big|\\big| Y-\\hat{\\theta}X \\big|\\big| + \\alpha∑_{i=1}^{n} \\hat{|\\theta_i|}\n",
    "$$\n",
    "\n",
    "We are looking to minimize the cost fuction with respect to W and b\n",
    "Where $\\hat{\\theta} = \\begin{bmatrix}  b & {\\hat{\\theta_1}} &.&.&.& {\\hat{\\theta_n}} \\end{bmatrix}$\n",
    "and $W = \\begin{bmatrix} \\hat{\\theta}_1 &.&.&.& \\hat{\\theta_n} \\end{bmatrix}$\n",
    "\n",
    "so $\\frac{d}{dW}J(\\hat{\\theta}) = \\frac{1}{m}X^T(Y-\\hat{\\theta}X) + \\alpha sgn(W)$\n",
    "\n",
    "and $\\frac{d}{db}J(\\hat{\\theta}) = \\frac{1}{m}(Y-\\hat{\\theta}X)$\n",
    "\n",
    "Then using the gradient descent:\n",
    "$$\n",
    "\\theta_{k+1}=\\theta_{k}-h \\nabla J\\left(\\theta_{k}\\right)\n",
    "$$\n",
    "\n",
    "Where $\\theta_{k}$ is our initial guess and $\\theta_{k+1}$ is our new guess and $h$ is our step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10f9305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(X, y, alpha, num_iterations, learning_rate):\n",
    "\n",
    "    # Get m the number of observations and n the number of independent variables\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    W = np.zeros(n)\n",
    "\n",
    "    # Initialize intercept term\n",
    "    b = 0\n",
    "\n",
    "    # Performing gradient descent to find the minimum of out cost function\n",
    "    for iteration in range(num_iterations):\n",
    "        predictions = np.dot(X, W) + b\n",
    "        residuals = predictions - y\n",
    "        dJdW = (1/m)*np.dot(X.T, residuals) + alpha * np.sign(W)\n",
    "        dJdb = (1/m)*np.sum(residuals)\n",
    "\n",
    "        W -= learning_rate * dJdW\n",
    "        b -= learning_rate * dJdb\n",
    "\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39c7b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.35748302983783, 12.37024224736695, 22.463406393328082, 54.92725786833492, 67.10327882472711, 135.2211408496511, 518498.2318359896]\n"
     ]
    }
   ],
   "source": [
    "las_results = []\n",
    "\n",
    "for alpha in test_alphas:\n",
    "  w, b = lasso_regression(X_train, y_train, alpha,num_iterations = 100000, learning_rate = 0.05)\n",
    "  Y_pred = np.dot(X_test, w) + b\n",
    "\n",
    "  las_results.append(mean_squared_error(y_test, Y_pred))\n",
    "\n",
    "print(las_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b47094ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11.413035283805366\n",
      "Mean Squared Error: 12.901685764529885\n",
      "Mean Squared Error: 22.456548850036334\n",
      "Mean Squared Error: 55.31351982194934\n",
      "Mean Squared Error: 75.76013971789499\n",
      "Mean Squared Error: 75.76013971789499\n",
      "Mean Squared Error: 75.76013971789499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for alpha in test_alphas:\n",
    "    lasso_model = Lasso(alpha=alpha,max_iter = 100000)\n",
    "\n",
    "    # Train the model\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ceb8dc",
   "metadata": {},
   "source": [
    "We can see that up to alpha = 10 the MSE of our gradient descent is approximately equal to the sklearn LASSO MSE. This will differ by changing up the number of iterations and learning rate.\n",
    "\n",
    "Also alpha = 0.001 or 0.01 seems to minimize the MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e207713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.05065185e-03 -1.30786474e-04  1.33800659e-03  2.89429150e-03\n",
      "  2.73764867e-03  8.84182765e-03 -1.54815999e-03 -8.63094201e+00\n",
      "  1.32791234e+01  9.55667847e-03  2.14979186e-03  4.19764849e+00\n",
      " -6.85762349e+00 -4.63692482e-04 -1.86216142e-04 -9.35529136e-04\n",
      " -6.00485784e-04 -1.05654121e-03  1.46659907e-03 -2.51719760e-03\n",
      " -1.81941515e-03 -8.09130848e+00 -3.15661566e-03 -8.94036386e-04\n",
      " -3.45122570e-03 -2.22740287e-04  1.80581780e+00 -1.62805211e-03\n",
      "  1.03268103e-03 -1.80140876e-03  3.51369019e-03  3.16503774e-04\n",
      "  1.91503091e-04 -1.07491103e-03  7.81459824e-04  1.44159791e-03\n",
      "  1.16219890e-03 -1.56990382e-03  2.18878756e-02  1.67475186e-03\n",
      "  2.21389775e-03  1.32967725e-03  7.09162498e-03 -7.95280675e-01\n",
      "  6.03134171e-03  7.83502016e+00 -3.75918116e-03  6.89980854e-03\n",
      " -3.76016918e-05  2.89429150e-03 -6.48213890e+00 -2.22383922e+00\n",
      "  1.43351372e-03  1.12349763e-03  6.97753137e-04  9.61843749e+00\n",
      "  1.05470690e-04  2.20876328e+00  2.29266121e-04 -2.48713144e+00\n",
      " -3.09323558e+00 -1.59183829e-03 -1.25676189e-03 -3.59282200e-05\n",
      " -1.08211790e-03 -4.73416243e+00  3.67942588e-03 -6.94777223e-04\n",
      "  3.70381016e+01 -6.34780919e-01  3.80406616e-03 -2.10880669e+01\n",
      " -9.96762804e+00 -7.05916529e+00  1.51027443e+00  4.45601468e-04\n",
      "  1.02327299e-02 -1.82181418e+00  7.23671941e+00  6.00478842e-03\n",
      " -1.45926234e+00 -6.43736544e-04 -8.84050144e+00  9.83366693e-04\n",
      " -1.85603329e-05  2.73256214e-04  1.39739994e-03  2.96523991e-04\n",
      " -1.07653061e-04  7.81499137e-03  8.31458907e-03  4.86580644e-03\n",
      "  1.43432176e+00 -1.51581589e+01  6.85886437e-03  4.92662955e-03\n",
      "  5.44466284e-03 -1.33568534e+01  8.22065898e-03  9.15026123e-03\n",
      "  2.11520011e-03  1.15686572e-02 -7.86835969e+00  2.12180860e+01]\n",
      "19.302661954296145\n"
     ]
    }
   ],
   "source": [
    "w, b = lasso_regression(X_train, y_train, alpha=0.01,num_iterations = 200000, learning_rate = 0.2)\n",
    "\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bafa3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -8.63094201e+00\n",
      "  1.32791234e+01  0.00000000e+00  0.00000000e+00  4.19764849e+00\n",
      " -6.85762349e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -8.09130848e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.80581780e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  2.18878756e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.95280675e-01\n",
      "  0.00000000e+00  7.83502016e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.48213890e+00 -2.22383922e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  9.61843749e+00\n",
      "  0.00000000e+00  2.20876328e+00  0.00000000e+00 -2.48713144e+00\n",
      " -3.09323558e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -4.73416243e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.70381016e+01 -6.34780919e-01  0.00000000e+00 -2.10880669e+01\n",
      " -9.96762804e+00 -7.05916529e+00  1.51027443e+00  0.00000000e+00\n",
      "  1.02327299e-02 -1.82181418e+00  7.23671941e+00  0.00000000e+00\n",
      " -1.45926234e+00  0.00000000e+00 -8.84050144e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.43432176e+00 -1.51581589e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.33568534e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.15686572e-02 -7.86835969e+00  2.12180860e+01]\n"
     ]
    }
   ],
   "source": [
    "# Get ride of small values close to 0\n",
    "for i in range(len(w)):\n",
    "    if(abs(w[i]) < 0.01):\n",
    "        w[i] = 0\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "571817fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -8.61448752e+00\n",
      "  1.32747817e+01  0.00000000e+00 -0.00000000e+00  4.22948747e+00\n",
      " -6.79244783e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -8.10113946e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  1.80657247e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  7.84771010e-03  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -8.13870499e-01\n",
      "  0.00000000e+00  7.87205046e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -6.47720077e+00 -2.21214790e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  9.61987786e+00\n",
      "  0.00000000e+00  2.20457393e+00 -0.00000000e+00 -2.49419166e+00\n",
      " -3.13735059e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -4.71374387e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  3.70600056e+01 -6.22026316e-01  0.00000000e+00 -2.11053722e+01\n",
      " -9.95546028e+00 -7.05137004e+00  1.47924858e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -1.85318578e+00  7.23916984e+00  0.00000000e+00\n",
      " -1.47972146e+00 -0.00000000e+00 -8.82302770e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.43952155e+00 -1.51421719e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.34045830e+01  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -7.91571342e+00  2.11787418e+01]\n",
      "19.265485274258005\n"
     ]
    }
   ],
   "source": [
    "# Create a Lasso regression model\n",
    "lasso_model = Lasso(alpha=0.01,max_iter = 1000000)\n",
    "# Fit the model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "print(lasso_model.coef_)\n",
    "print(lasso_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c416877",
   "metadata": {},
   "source": [
    "If we compare the coefficients of sklearn LASSO to the gradient descent, the gradient descent has almost the same values as the sklearn one\n",
    "\n",
    "But notice that we had to change the number of iterations and learning rate to find a close approximate to the sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10092e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.890890065621804\n"
     ]
    }
   ],
   "source": [
    "Y_pred_test = np.dot(X_test, w) + b\n",
    "print(mean_squared_error(y_test, Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62f5d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.514728621185105\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = np.dot(X_train, w) + b\n",
    "print(mean_squared_error(y_train, Y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c71b7",
   "metadata": {},
   "source": [
    "The MSE now for our LASSO function is 12.295366661959434 now further from the SKlearn MSE of 12.901685764529885 but the coeffcients are now closer.\n",
    "\n",
    "Our training data MSE and test MSE now seem to be much closer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
